import { checkRateLimit, addRateLimitHeaders } from '../lib/rateLimit.js';
import { handleCORS } from '../lib/corsConfig.js';
import { SILENCE_2S_BASE64 } from './silence.js';

/**
 * ARCHITECTURE DE SEGMENTATION AUDIO POUR STABILIT√â VOCALE
 *
 * Probl√®me r√©solu: Sur les longs textes (>800 caract√®res), ElevenLabs peut:
 * - Changer d'accent (qu√©b√©cois ‚Üí fran√ßais)
 * - Modifier le ton (calme ‚Üí √©nergique)
 * - Acc√©l√©rer vers la fin
 *
 * Solution: Segmentation par paragraphes + Request Stitching + Concat√©nation
 *
 * 1. D√©coupe le texte en paragraphes (2-3 segments)
 * 2. G√©n√®re l'audio de chaque segment avec previous_request_ids
 * 3. Ins√®re 4 secondes de silence entre chaque segment
 * 4. Concat√®ne tous les buffers MP3
 *
 * Sources:
 * - https://elevenlabs.io/docs/cookbooks/text-to-speech/request-stitching
 * - https://help.elevenlabs.io/hc/en-us/articles/19631995406481
 */

// ============================================================================
// CONFIGURATION
// ============================================================================

// Voix ElevenLabs
const VOICE_IDS = {
  meditation: 'xsNzdCmWJpYoa80FaXJi',  // Iza - voix f√©minine qu√©b√©coise
  reflection: '93nuHbke4dTER9x2pDwE'   // Dann - voix masculine
};

// Contexte √©motionnel pour ancrer l'accent et le ton
// IMPORTANT: Mention explicite "qu√©b√©coise" pour stabiliser l'accent
const EMOTIONAL_CONTEXT = {
  meditation: {
    previous_text: `La guide qu√©b√©coise ferme les yeux, respirant lentement.`,
    next_text: `, murmure-t-elle tout bas, gardant son calme jusqu'√† la fin.`
  },
  reflection: {
    previous_text: `Il te regarde avec bienveillance.`,
    next_text: `, dit-il d'un ton pos√© et r√©fl√©chi.`
  }
};

// Voice settings optimis√©s pour stabilit√© maximale
const VOICE_SETTINGS = {
  meditation: {
    stability: 0.95,           // MAXIMUM - accent qu√©b√©cois ultra-stable
    similarity_boost: 0.95,    // MAXIMUM - fid√©lit√© totale √† la voix originale
    style: 0.0,                // Z√âRO - aucune variation stylistique
    speed: 0.72,               // Lent pour m√©ditation
    use_speaker_boost: true
  },
  reflection: {
    stability: 0.70,
    similarity_boost: 0.85,
    style: 0.05,
    speed: 0.82,
    use_speaker_boost: true
  }
};

// Dur√©e du silence entre paragraphes (en secondes)
const SILENCE_DURATION_SECONDS = 4;

// ============================================================================
// UTILITAIRES
// ============================================================================

/**
 * Buffer de silence MP3 (2 secondes, 44.1kHz)
 * Stock√© en base64 pour compatibilit√© Vercel Serverless
 */
const silenceBuffer = Buffer.from(SILENCE_2S_BASE64, 'base64');

// ============================================================================
// CONCAT√âNATION MP3 PAR FRAMES
// ============================================================================

/**
 * Trouve le d√©but des donn√©es audio MP3 (apr√®s les tags ID3v2)
 * Les frames MP3 commencent par le sync word 0xFF 0xFB/0xFA/0xF3/0xF2
 */
function findMP3DataStart(buffer) {
  let offset = 0;

  // V√©rifier si ID3v2 tag pr√©sent
  if (buffer[0] === 0x49 && buffer[1] === 0x44 && buffer[2] === 0x33) { // "ID3"
    // Lire la taille du tag ID3v2 (syncsafe integer sur 4 bytes)
    const size = (buffer[6] << 21) | (buffer[7] << 14) | (buffer[8] << 7) | buffer[9];
    offset = 10 + size;
  }

  // Chercher le premier frame sync (0xFF suivi de 0xFB, 0xFA, 0xF3, 0xF2, ou 0xE*)
  while (offset < buffer.length - 1) {
    if (buffer[offset] === 0xFF && (buffer[offset + 1] & 0xE0) === 0xE0) {
      return offset;
    }
    offset++;
  }

  return 0; // Fallback au d√©but
}

/**
 * Trouve la fin des donn√©es audio MP3 (avant les tags ID3v1)
 */
function findMP3DataEnd(buffer) {
  let end = buffer.length;

  // V√©rifier si ID3v1 tag pr√©sent (128 derniers bytes commen√ßant par "TAG")
  if (end > 128) {
    const tagStart = end - 128;
    if (buffer[tagStart] === 0x54 && buffer[tagStart + 1] === 0x41 && buffer[tagStart + 2] === 0x47) {
      end = tagStart;
    }
  }

  return end;
}

/**
 * Extrait uniquement les frames audio MP3 (sans headers/tags)
 */
function extractMP3Frames(buffer) {
  const start = findMP3DataStart(buffer);
  const end = findMP3DataEnd(buffer);
  return buffer.slice(start, end);
}

/**
 * Concat√®ne plusieurs buffers MP3 de mani√®re propre
 *
 * IMPORTANT: Cette m√©thode fonctionne car ElevenLabs g√©n√®re des MP3 avec
 * les m√™mes param√®tres (44.1kHz, 192kbps, st√©r√©o). Les frames sont donc
 * compatibles et peuvent √™tre concat√©n√©es directement.
 *
 * @param buffers - Array de buffers MP3
 * @returns Buffer MP3 concat√©n√©
 */
function concatenateMP3Buffers(buffers) {
  if (buffers.length === 0) return Buffer.alloc(0);
  if (buffers.length === 1) return buffers[0];

  console.log(`üîó Concatenating ${buffers.length} MP3 buffers...`);

  // Extraire les frames de chaque buffer
  const frameBuffers = buffers.map((buf, idx) => {
    const frames = extractMP3Frames(buf);
    console.log(`   Buffer ${idx + 1}: ${buf.length} bytes ‚Üí ${frames.length} bytes (frames only)`);
    return frames;
  });

  // Concat√©ner tous les frames
  const result = Buffer.concat(frameBuffers);
  console.log(`   Total: ${result.length} bytes`);

  return result;
}

/**
 * D√©coupe le texte en segments pour la g√©n√©ration audio
 *
 * Strat√©gie: Regrouper les paragraphes en 3-5 segments pour:
 * - √âviter trop d'appels API (co√ªteux et lents)
 * - Garder des segments assez courts pour la stabilit√© vocale (~800-1200 chars)
 * - Permettre une concat√©nation propre
 *
 * MAX_SEGMENTS: 5 segments maximum
 * TARGET_SEGMENT_SIZE: ~1000 caract√®res par segment
 */
const MAX_SEGMENTS = 5;
const TARGET_SEGMENT_SIZE = 1000;

function splitIntoSegments(text) {
  // S√©parer par double saut de ligne (paragraphes)
  const paragraphs = text.split(/\n\n+/).filter(p => p.trim());

  if (paragraphs.length === 0) {
    return [text];
  }

  // Si le texte est court, pas besoin de segmentation
  if (text.length < 800) {
    return [text];
  }

  // Calculer le nombre id√©al de segments
  const idealSegments = Math.min(MAX_SEGMENTS, Math.ceil(text.length / TARGET_SEGMENT_SIZE));

  // Si on a peu de paragraphes, les garder tels quels
  if (paragraphs.length <= idealSegments) {
    return paragraphs;
  }

  // Regrouper les paragraphes pour avoir ~idealSegments segments
  const paragraphsPerSegment = Math.ceil(paragraphs.length / idealSegments);
  const segments = [];

  for (let i = 0; i < paragraphs.length; i += paragraphsPerSegment) {
    const segmentParagraphs = paragraphs.slice(i, i + paragraphsPerSegment);
    segments.push(segmentParagraphs.join('\n\n'));
  }

  return segments;
}

/**
 * Pr√©pare un segment de texte pour ElevenLabs
 * - Nettoie les ellipses
 * - Entoure de guillemets (technique dialogue lu)
 */
function prepareSegment(text) {
  let processed = text.trim();
  processed = processed.replace(/\.\.\./g, '... ');
  processed = processed.replace(/\n/g, '. '); // Lignes simples ‚Üí pause courte
  return `"${processed}"`;
}

// ============================================================================
// G√âN√âRATION AUDIO SEGMENT√âE
// ============================================================================

/**
 * G√©n√®re l'audio d'un segment avec ElevenLabs
 *
 * Utilise previous_request_ids pour le Request Stitching quand disponible.
 * Cela permet √† ElevenLabs de maintenir la continuit√© vocale entre segments.
 *
 * @param previousRequestIds - Array des IDs de requ√™tes pr√©c√©dentes (max 3)
 * @returns {audioBuffer, requestId} - Buffer audio et ID de la requ√™te
 */
async function generateSegmentAudio(
  text,
  voiceId,
  voiceSettings,
  emotionalContext,
  segmentIndex,
  totalSegments,
  previousRequestIds = []
) {
  console.log(`  üì§ Calling ElevenLabs API for segment ${segmentIndex + 1}/${totalSegments}...`);
  console.log(`  üìù Text length: ${text.length} chars`);
  if (previousRequestIds.length > 0) {
    console.log(`  üîó Using ${previousRequestIds.length} previous request ID(s) for stitching`);
  }

  const requestBody = {
    text: text,
    model_id: 'eleven_multilingual_v2',
    language_code: 'fr',
    voice_settings: voiceSettings,
    seed: 42,
    output_format: 'mp3_44100_192'
  };

  // Pour le premier segment, utiliser le contexte √©motionnel
  // Pour les suivants, utiliser previous_request_ids pour le stitching
  if (segmentIndex === 0) {
    requestBody.previous_text = emotionalContext.previous_text;
    requestBody.next_text = emotionalContext.next_text;
  } else if (previousRequestIds.length > 0) {
    // Utiliser les 3 derniers IDs maximum
    requestBody.previous_request_ids = previousRequestIds.slice(-3);
  }

  const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`, {
    method: 'POST',
    headers: {
      'Accept': 'audio/mpeg',
      'Content-Type': 'application/json',
      'xi-api-key': process.env.ELEVENLABS_API_KEY
    },
    body: JSON.stringify(requestBody)
  });

  if (!response.ok) {
    const errorBody = await response.text();
    console.error(`  ‚ùå ElevenLabs API error for segment ${segmentIndex + 1}:`, response.status, errorBody);
    throw new Error(`ElevenLabs API error: ${response.status} - ${errorBody}`);
  }

  // R√©cup√©rer le request_id pour le stitching des segments suivants
  const requestId = response.headers.get('request-id');
  console.log(`  üÜî Request ID: ${requestId}`);

  const audioBuffer = Buffer.from(await response.arrayBuffer());
  console.log(`  ‚úÖ Segment ${segmentIndex + 1} generated: ${audioBuffer.length} bytes`);

  return { audioBuffer, requestId };
}

/**
 * G√©n√®re l'audio complet avec segmentation et concat√©nation MP3
 *
 * ARCHITECTURE:
 * 1. D√©coupe le texte en 3-5 segments (~800-1200 chars chacun)
 * 2. G√©n√®re chaque segment avec ElevenLabs (stabilit√© vocale)
 * 3. Utilise Request Stitching pour continuit√© entre segments
 * 4. Concat√®ne les frames MP3 (sans les headers/tags)
 * 5. Ajoute des silences de 4s entre segments
 *
 * Cette approche garantit:
 * - Stabilit√© de l'accent qu√©b√©cois sur les longs textes
 * - Audio complet jouable sans coupure
 * - Pauses naturelles entre les sections
 */
async function generateSegmentedAudio(text, guideType) {
  const isMeditation = guideType !== 'reflection';
  const type = isMeditation ? 'meditation' : 'reflection';

  const voiceId = VOICE_IDS[type];
  const voiceSettings = VOICE_SETTINGS[type];
  const emotionalContext = EMOTIONAL_CONTEXT[type];

  console.log(`üìù Total text length: ${text.length} chars`);
  console.log(`üé≠ Voice type: ${type}`);

  // D√©couper en segments (3-5 max pour stabilit√© vocale)
  const segments = splitIntoSegments(text);
  console.log(`üì¶ Split into ${segments.length} segment(s)`);
  segments.forEach((s, i) => {
    console.log(`   Segment ${i + 1}: ${s.length} chars`);
  });

  // Si un seul segment court, g√©n√©ration directe
  if (segments.length === 1 && text.length < 1500) {
    console.log(`üéôÔ∏è Single segment mode - direct generation`);
    const preparedText = prepareSegment(segments[0]);
    const { audioBuffer } = await generateSegmentAudio(
      preparedText,
      voiceId,
      voiceSettings,
      emotionalContext,
      0,
      1,
      []
    );
    console.log(`‚úÖ Audio generated: ${audioBuffer.length} bytes`);
    return audioBuffer;
  }

  // G√©n√©ration multi-segments avec Request Stitching
  console.log(`üîä Multi-segment generation with Request Stitching...`);

  const audioBuffers = [];
  const requestIds = [];
  const silenceFrames = extractMP3Frames(silenceBuffer);

  for (let i = 0; i < segments.length; i++) {
    console.log(`\nüéôÔ∏è === SEGMENT ${i + 1}/${segments.length} ===`);

    const preparedText = prepareSegment(segments[i]);
    console.log(`   Text: ${preparedText.length} chars`);

    try {
      const { audioBuffer, requestId } = await generateSegmentAudio(
        preparedText,
        voiceId,
        voiceSettings,
        emotionalContext,
        i,
        segments.length,
        requestIds
      );

      // Sauvegarder le requestId pour le stitching
      if (requestId) {
        requestIds.push(requestId);
      }

      // Ajouter les frames audio
      audioBuffers.push(audioBuffer);
      console.log(`   ‚úÖ Generated: ${audioBuffer.length} bytes`);

      // Ajouter silence entre segments (sauf apr√®s le dernier)
      if (i < segments.length - 1) {
        // 4 secondes = 2x silence de 2s
        audioBuffers.push(silenceBuffer);
        audioBuffers.push(silenceBuffer);
        console.log(`   ‚è∏Ô∏è Added ${SILENCE_DURATION_SECONDS}s silence`);
      }

    } catch (error) {
      console.error(`   ‚ùå Failed: ${error.message}`);
      throw error;
    }
  }

  // Concat√©ner tous les buffers MP3 proprement
  console.log(`\nüîó Final concatenation...`);
  const finalAudio = concatenateMP3Buffers(audioBuffers);
  console.log(`‚úÖ Final audio: ${finalAudio.length} bytes (${(finalAudio.length / 1024).toFixed(1)} KB)`);

  return finalAudio;
}

// ============================================================================
// HANDLER PRINCIPAL
// ============================================================================

export default async function handler(req, res) {
  // üîê CORS S√âCURIS√â
  if (!handleCORS(req, res)) {
    return;
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  // ‚ö° RATE LIMITING
  const rateLimit = checkRateLimit(req, '/api/audio');
  addRateLimitHeaders(res, { ...rateLimit, endpoint: '/api/audio' });

  if (!rateLimit.allowed) {
    console.warn(`üö´ Rate limit exceeded for audio`);
    return res.status(429).json({
      error: 'Too Many Requests',
      message: rateLimit.message,
      retryAfter: rateLimit.retryAfter
    });
  }

  try {
    const { text, guideType } = req.body;

    if (!text) {
      return res.status(400).json({ error: 'Missing text field' });
    }

    console.log('=== SEGMENTED AUDIO GENERATION ===');
    console.log('Guide Type:', guideType);
    console.log('Text length:', text.length, 'characters');

    // G√©n√©rer l'audio avec segmentation
    const audioBuffer = await generateSegmentedAudio(text, guideType);

    // Convertir en base64
    const base64Audio = audioBuffer.toString('base64');

    console.log('=== GENERATION COMPLETE ===');

    res.status(200).json({ audio: base64Audio });

  } catch (error) {
    console.error('Error generating audio:', error);
    res.status(500).json({ error: 'Failed to generate audio', details: error.message });
  }
}
